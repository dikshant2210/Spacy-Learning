{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Hello, world. Welcome to natural language processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\nHello, world.\nWelcome to natural language processing.\n"
     ]
    }
   ],
   "source": [
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello - INTJ\n, - PUNCT\nworld - NOUN\n. - PUNCT\nWelcome - VERB\nto - ADP\nnatural - ADJ\nlanguage - NOUN\nprocessing - NOUN\n. - PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello --> []\n, --> [,, Hello]\nworld --> [world, Hello]\n. --> [., Hello]\nWelcome --> []\nto --> [to, Welcome]\nnatural --> [natural, processing, processing, to, to, Welcome]\nlanguage --> [language, processing, processing, to, to, Welcome]\nprocessing --> [processing, to, to, Welcome]\n. --> [., Welcome]\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_root(token):\n",
    "    token_to_r = list()\n",
    "    while token.head is not token:\n",
    "        token_to_r.append(token)\n",
    "        token = token.head     \n",
    "        token_to_r.append(token)\n",
    "    return token_to_r\n",
    "\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n,-punct-> Hello-ROOT\nworld-npadvmod-> Hello-ROOT\n.-punct-> Hello-ROOT\n\nto-prep-> Welcome-ROOT\nnatural-amod-> processing-pobj-> processing-pobj-> to-prep-> to-prep-> Welcome-ROOT\nlanguage-compound-> processing-pobj-> processing-pobj-> to-prep-> to-prep-> Welcome-ROOT\nprocessing-pobj-> to-prep-> to-prep-> Welcome-ROOT\n.-punct-> Welcome-ROOT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) \n",
    "                      for dependent_token in tokens_to_root(token)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack - PERSON\n"
     ]
    }
   ],
   "source": [
    "doc_2 = nlp(u'I went to delhi where I met my old friend Jack from uni.')\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , -4.064180850982666\nwent , -8.474893569946289\nto , -3.83851957321167\ndelhi , -19.579313278198242\nwhere , -7.183883190155029\nI , -4.064180850982666\nmet , -9.784490585327148\nmy , -5.918124675750732\nold , -7.7954816818237305\nfriend , -8.825821876525879\nJack , -11.20296573638916\nfrom , -6.028810501098633\nuni , -19.579313278198242\n. , -3.0729479789733887\n"
     ]
    }
   ],
   "source": [
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n0.0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.569403101179\n0.323890751106\n"
     ]
    }
   ],
   "source": [
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
